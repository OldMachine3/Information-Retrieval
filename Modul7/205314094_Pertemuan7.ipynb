{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from importlib import import_module\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import copy\n",
    "import string\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# !pip install ipython-autotime\n",
    "\n",
    "# %load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ekstrak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'doc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for (dirpath, dirnames, filenames)in os.walk(str(os.getcwd())+'/'+title+'/'):\n",
    "    for i in filenames:\n",
    "        paths.append(str(dirpath)+str(\"/\")+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header(data):\n",
    "    try:\n",
    "        ind = data.index('\\n\\n')\n",
    "        data = data[ind:]\n",
    "    except:\n",
    "        print(\"No Header\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_characters(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    data = np.char.replace(data, \"0\", \" zero \")\n",
    "    data = np.char.replace(data, \"1\", \" one \")\n",
    "    data = np.char.replace(data, \"2\", \" two \")\n",
    "    data = np.char.replace(data, \"3\", \" three \")\n",
    "    data = np.char.replace(data, \"4\", \" four \")\n",
    "    data = np.char.replace(data, \"5\", \" five \")\n",
    "    data = np.char.replace(data, \"6\", \" six \")\n",
    "    data = np.char.replace(data, \"7\", \" seven \")\n",
    "    data = np.char.replace(data, \"8\", \" eight \")\n",
    "    data = np.char.replace(data, \"9\", \" nine \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, query):\n",
    "    if not query:\n",
    "        data = remove_header(data)        \n",
    "    data = convert_lower_case(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_stop_words(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_single_characters(data)\n",
    "    data = stemming(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buat Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = pd.DataFrame()\n",
    "frequency = pd.DataFrame()\n",
    "doc = 0\n",
    "\n",
    "for path in paths:\n",
    "    preprocessed_text = preprocess(path, False)\n",
    "    if doc%100 == 0:\n",
    "        print(doc)\n",
    "    tokens = word_tokenize(str(preprocessed_text))\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in postings:\n",
    "            p = postings[token][0]\n",
    "\n",
    "            k = [a[0] for a in p]\n",
    "\n",
    "            if doc in k:\n",
    "                for a in p:\n",
    "                    if a[0] == doc:\n",
    "                        a[1].add(pos)\n",
    "            else:\n",
    "                p.append([doc,{pos}])\n",
    "                frequency[token][0] += 1\n",
    "        else:\n",
    "            postings.insert(value=[[[doc, {pos}]]], loc=0, column=token)\n",
    "            frequency.insert(value=[1], loc=0, column=token)\n",
    "        pos += 1\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_postings(word):\n",
    " preprocessed_word = str(preprocess(word, True))\n",
    " print(preprocessed_word)\n",
    " print(\"Frequency:\",frequency[preprocessed_word][0])\n",
    " print(\"Postings List:\",postings[preprocessed_word][0])\n",
    " total=0\n",
    " #total\n",
    " for x in postings[preprocessed_word][0]:\n",
    "    total+=len(x)\n",
    " #Print probability\n",
    " i=1\n",
    " for x in postings[preprocessed_word][0]:\n",
    "    prob=len(x)/total\n",
    " print(\"Document \",i,\":\",prob)\n",
    " i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "def zip_jm(x, lambda_=lambda_):\n",
    " if lambda_ < 0 or lambda_ > 1 :\n",
    "    return np.zeros_like(x)\n",
    " else:\n",
    "    return (x == 0) * (1-lambda_) * (kata/d) + lambda_ * c\n",
    "\n",
    "\n",
    "\n",
    "zip_jm(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "xs = np.arange(0, 10);\n",
    "palette = sns.color_palette()\n",
    "ax.bar(2.5 * xs, stats.poisson.pmf(xs, lambda_), width=0.9,\n",
    "color=palette[0], label='Poisson');\n",
    "ax.bar(2.5 * xs + 1, zip_pmf(xs), width=0.9, color=palette[1],\n",
    "label='Zero-inflated Poisson');\n",
    "ax.set_xticks(2.5 * xs + 1);\n",
    "ax.set_xticklabels(xs);\n",
    "ax.set_xlabel('$x$');\n",
    "ax.set_ylabel('$P(X = x)$');\n",
    "ax.legend();\n",
    "N = 1000\n",
    "inflated_zero = stats.bernoulli.rvs(pi, size=N)\n",
    "x = (1 - inflated_zero) * stats.poisson.rvs(lambda_, size=N)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(x, width=0.8, bins=np.arange(x.max() + 1), normed=True);\n",
    "ax.set_xticks(np.arange(x.max() + 1) + 0.4);\n",
    "ax.set_xticklabels(np.arange(x.max() + 1));\n",
    "ax.set_xlabel('$x$');\n",
    "ax.set_ylabel('Proportion of samples');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81ac62153dbc0a1bc311088bfbe972f5bbc7cf4d6a509b29cbf21106c5927a31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
